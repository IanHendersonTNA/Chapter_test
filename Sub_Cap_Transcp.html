<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
 
 <!--CSS -->
 
 <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
 <link rel="stylesheet" href="style.css">
 
 <link rel="stylesheet" href="style.css">
  <style>
    html {
      --scrollbarBG: #8BA4AE;
      --thumbBG: #DEE5E8;
    }

    body::-webkit-scrollbar {
      width: 15px;
    }

    body::-webkit-scrollbar-track {
      background: var(--scrollbarBG);
    }

    body::-webkit-scrollbar-thumb {
      background-color: var(--thumbBG);
      border-radius: 20px;
      border: 1px solid var(--scrollbarBG);
    }
      
       .float-left-text {
      float: left;
        position: relative;
        /*padding-right: 20px;*/
        margin-right: 10px;
       width: 380px;
       height: 200px;
}


  </style>
  <!--CSS -->
    <!--BODY-->
  <meta charset="utf-8">
  <title></title>
</head>

<body style="font-family: sans-serif;">
  <div class="page">
    <!--NAVBAR-->
  
    
     
       <div class="w3-top">
<div class="navbar w3-blue-grey">
  <a href="index.html" class="w3-bar-item w3-button w3-mobile navbar-link">Home</a>
  
  <!--<div class="w3-dropdown-hover w3-mobile navbar-link">
    <button class="w3-button">Research Proposals <i class="fa fa-caret-down"></i></button>
    <div class="w3-dropdown-content w3-bar-block w3-dark-grey">
        <a href="Research_Proposal_Transcription.html" class="w3-bar-item w3-button w3-mobile navbar-link">AV Transcription</a>
        <a href="Research_Proposal_Chaptering.html" class="w3-bar-item w3-button w3-mobile navbar-link">AV Chaptering</a>
        <a href="Work_Plan.html" class="w3-bar-item w3-button w3-mobile navbar-link">Work Plan</a>
      </div>
    </div>
    -->
    
     <a href="Research_Project_Overview.html" class="w3-bar-item w3-button w3-mobile navbar-link">Project Overview</a>
    
<!--
    
  <a href="HowSpeechToTextWorks.html" class="w3-bar-item w3-button w3-mobile navbar-link">Speech to Text</a>
-->
   
   
    <div class="w3-dropdown-hover w3-mobile navbar-link">
    <button class="w3-button navbar-link-dropdown">Speech to Text <i class="fa fa-caret-down"></i></button>
    <div class="w3-dropdown-content w3-bar-block w3-dark-grey">
        <a href="HowSpeechToTextWorks.html" class="w3-bar-item w3-button w3-mobile navbar-link">Speech &amp; Speech Recogntion</a>
        <a href="Automated_Transcription_Service_Testing.html" class="w3-bar-item w3-button w3-mobile navbar-link">Testing Transcription Services</a>
       <!-- <a href="Work_Plan.html" class="w3-bar-item w3-button w3-mobile navbar-link">Work Plan</a>-->
      </div>
    </div>
   
   
   
   
   
   <div class="w3-dropdown-hover w3-mobile navbar-link">
    <button class="w3-button navbar-link-dropdown">AV Transcription &amp; Chaptering <i class="fa fa-caret-down"></i></button>
    <div class="w3-dropdown-content w3-bar-block w3-dark-grey">
       
       
        <a href="Sub_Cap_Transcp.html" class="w3-bar-item w3-button w3-mobile navbar-link">Textural Representation of Speech</a>
        <a href="Player_Models.html" class="w3-bar-item w3-button w3-mobile navbar-link">AV Player Models</a>
       
       <!-- <a href="Work_Plan.html" class="w3-bar-item w3-button w3-mobile navbar-link">Work Plan</a>-->
      </div>
    </div>
 
   
<!--
   <div class="w3-dropdown-hover w3-mobile navbar-link">
    <button class="w3-button navbar-link-dropdown">Testing &amp; Models <i class="fa fa-caret-down"></i></button>
    <div class="w3-dropdown-content w3-bar-block w3-dark-grey">
        <a href="Automated_Transcription_Service_Testing.html" class="w3-bar-item w3-button w3-mobile navbar-link">Testing Services</a>
        <a href="Player_Models.html" class="w3-bar-item w3-button w3-mobile navbar-link">Player Models</a>
       <a href="Work_Plan.html" class="w3-bar-item w3-button w3-mobile navbar-link">Work Plan</a>
      </div>
    </div>
-->
   
   
   
   
   
   
   
    <!--<a href="Automated_Transcription_Services.html" class="w3-bar-item w3-button w3-mobile navbar-link">Transcription Services</a>
    
    <a href="index_RENAME.html" class="w3-bar-item w3-button w3-mobile navbar-link">Player Work</a>
    
  <a href="Work_Plan.html" class="w3-bar-item w3-button w3-mobile navbar-link">Work Plan</a>-->
</div>
   </div>
   <!--NAVBAR-->
   
   
 <!--   <div class="w3-top">
    <div class="navbar w3-blue-grey">
    <a href="index.html" class="w3-bar-item w3-button w3-mobile navbar-link w3-black">Home</a>
    
     <div class="w3-dropdown-hover w3-mobile navbar-link">
      <button class="w3-button dropdown-link">Digital Formats<i class="fa fa-caret-down"></i></button>
      <div class="w3-dropdown-content w3-bar-block w3-dark-grey navbar-link">
        <a href="Apple_ProRes_LT.html" class="w3-bar-item w3-button w3-mobile navbar-link">Apple ProRes LT</a>
        <a href="Work_Plan.html" class="w3-bar-item w3-button w3-mobile navbar-link">---2</a>
        <a href="#" class="w3-bar-item w3-button w3-mobile navbar-link">---3</a>
      </div>
      </div>-->
   
   <div id="flex-parent" class="double-border">
   
    <h1>Textural Representation of Speech for AV:</h1>
    
    <h2>Subtitles, Captions &amp; Transcripts</h2>
        <p class="explain">Page content best viewed at a browser width of 980px or higher</p>

      <!--CAPTIONS AND SUBTITLES--> 
 
   
   
   
    
    <h3>Introduction</h3>
    
    <h4>Audio and Radio</h4>
    
    <p>Basic audio recordings could be made, and the methods of capture and reproduction had reached a  relative stability along with widespread distribution at a time when the capture and render of moving images were still in early development. Upon reaching the age of broadcast, audio became a major international source of information, directly available to the hearing-public in their homes and at places of work. Until television with broadcast subtitles became widely available there was little in the way of support for hearing-impaired audiences. There were some text-based broadcast technologies available in a very limited capacity. Such technologies were mainly used for military or scientific purposes and not applied to broadcast accessibility. One system developed in the U.S. was '<em>Radioteletype</em>' which involved the use of electromechanical teleprinters that were connected by radio.</p>
    
    <h4>Silent Cinema Intertitles</h4>
    <p>Title-cards or <em>'intertitles'</em> which generally presented quite limited portions of text representing passages of speech were necessarily employed from the start of silent cinema up until the full conversion to audio sound-stages and cinema-houses from the late 1920's. These were normally inserted between scenes, cutting into the visual action. They were also used for the presentation of movies in other languages, in this capacity they functioned as <strong><em>subtitles</em></strong> (- other language translations of dialogue). As no audio was involved to complicate the medium this allowed early cinema to become an international art-form - where the hearing-impaired were not excluded.</p>

   <h4>'Talkies' and Subtitles</h4>
     <p>With the advent of sound in 'talking pictures' disparate technological solutions were developed for the presentation of text alongside the visual content, creating the potential for movies to be distributed to all regions with the audio in the original language, with subtitles in the regional language, allowing access to both hearing and hearing impaired audiences. The full dialogue could be displayed without any interruption to visual action, the text synched to speech in the soundtrack.</p>
    
    <h4>Television</h4>
        <p>Due no doubt to technical limitations of the time but possibly without initial consideration, television on its arrival, as with the radio medium, largely excluded the hearing-impaired. Though they might view the (in early models very small) picture and perhaps lip-read performers displayed at low resolution, programmes would not necessarily be shot in a manner that would allow vision-only viewers to track all action and dialogue. In the 1970s there were some regional initiatives (e.g. for news broadcasts) where users, with special decoder units fitted, could access captions.</p>  
    
    <p>The number of programmes with text support was initially low. The first 'Teletext' captions were transmitted by 'Ceefax' in 1975 (<a href="http://teletext.mb21.co.uk/timeline/early-ceefax-subtitling.shtml">http://teletext.mb21.co.uk/timeline/early-ceefax-subtitling.shtml</a>) and the service gained poularity through the 1970s and 80s ('Oracle' from 1978 -1992, then Teletext). In 1990 Parliament passed the first piece of legislation covering 'subtitles' (more accurately 'captions') - the Broadcasting Act, Section 35. This required that public broadcasters <span class="quote">"provide minimum amounts of subtitling for deaf and hard-of-hearing people and to attain such technical standards in the provision of subtitling as the ITC (Independent Television Commission) specifies.”</span> The Technical Regulation Department of the ITC produces its 'Technical Performance Code' that governs technical standards for television. The Communications Act of 2003 expanded on requirements to provide accessibility services such as sign-language presentation.</p>
    
    <p>Though with the advent of the home VHS tape market in the 1980s movies for purchase or rent in that format might carry captions, the situation for those requiring text support improved with the: switch from analogue to digital television along with other technological developments in broadcasting, and the move from analogue tape to digital disk formats including DVD with its native text on screen support.</p>
    
      
<div class="explain"><h4>Captions vs. Subtitles</h4>
     <p>Within a digital audio-visual (AV) context captions and subtitles are both textural representations of speech and are designed to display in synch with the aligned part of speech within the audio of AV content. Though the intended applications differ there is some confusion with misapplication of the terms, and they are often used interchangeably. <strong><em>The distinction is:</em></strong></p>
       <ul>
           <li><span class="code-blue">Captions</span> are intended for use as a transcription of the dialogue in original language but may also include text descriptions of audible events represented e.g. an off-screen crash, or prominent music ('Audio Description')</li>
           <li><span class="code-blue">Subtitles</span> are intended to be used for language translations of dialogue but can also include audio-descriptive content</li>
       </ul>
       
       <p><strong>This from the Bureau of Internet Accessibility:</strong>
        <span class="quote">“Captions should not be confused with subtitles - they are similar, yet distinct from each other. While subtitles are a straightforward translation of the video’s dialogue, often times in a different language, captions not only have a text description of the spoken word but also include description of the background music or sound so as to provide the same level of information as one would get from hearing the audio."</span></p>
<p><span class="quote">"To meet web content accessibility standards, always include captions to pre-recorded videos or provide real-time captions for live videos. According to WebAIM, "captions should be: </span></p>
    <span class="quote"><ul><li>Synchronized - the text content should appear at approximately the same time that audio would be available</li>
        <li>Equivalent - content provided in captions should be equivalent to that of the spoken word</li>
        <li>Accessible - caption content should be readily accessible and available to those who need it."</li></ul></span>
        <p><a href="https://www.boia.org/blog/checklist-for-creating-accessible-videos">https://www.boia.org/blog/checklist-for-creating-accessible-videos</a></p>
       </div>
    
    
    <h4>Digital Video</h4>
    <p>Though video is considered principally a visual medium, it would be true to state that the audio component is generally of near equal import in the presentation of information, and in the case of many video files, where the primary information source is actually the audio as monologue or dialogue, then the rendering and representation of the recorded speech can become the focus for presentation considerations.</p>
    <p>Digital video in combination with playback via computer installed, or cloud-based digital media players carries the potential for comprehensive textural support.</p>
    <h5>Open Captions</h5>
    <p>One rather outmoded method is the presentation of captions or subtitles as <strong><em>'burnt-in'</em></strong> or <strong><em>'Open Captions'</em></strong>. These are permanently fixed text renders, applied to the video image. Burnt-in subtitles were useful for players that didn't support the display of timed text but this shouldn't now be an issue. They are otherwise inflexible and undesirable: </p>
    <ul>
    <li>they are not generally accessible to screen reading software</li>
    <li>they can obscure portions of visual action </li>
    <li>as fixed items they are uneditable in the case of error</li> 
    <li>they cannot be resized for presentation at different resolutions, and distort if displayed at incorrect aspect ratio, in each case they may become illegible</li>  
    <li>if transcoded to other copies textural edges may blur or block, and spaces fill or merge</li> 
    <li>in the scenario of producing multi-language versions an individual file would have to be produced for each language.</li></ul>
    <h5>Closed Captions</h5>
   <p><strong><em>'Closed Captions'</em></strong> are the standard way of applying text support in digital video, used by broadcasters, video streaming services, and individual content suppliers. The displayed text is content derived from a file separate from the AV file but linked. Closed Captions offer the benefit of flexibility in presentation:</p>
   <ul>
      <li>their display can be toggled on and off at viewer's choice (generally from a switch identified by a [CC] symbol).</li>
       <li>It may be possible for the viewer to alter the placement, font-size and font family of captions to avoid obscuring visual content and to optimise text clarity</li>
       <li>closed captions can be styled (by implemented standards) to highlight words of import, or to aid in the understanding of the context of individual parts of speech, i.e. to denote a particular speaker, to express vocal emphasis in text etc.</li>
      <li>as the displayed content comes from a separate file the AV file remains unaltered</li>
          <li>caption files can be a simple text-based files allowing for easy editing, and interoperable use in other services and scenarios</li>
       <li>the same text source can be used to generate both captions and transcripts, or otherwise transcripts can be generated from pre-existing captions</li>
        <li>the text can be used entirely independent of the AV as an alternative access manifestation</li>
         <li>(unless the text content is otherwise made inaccessible) the text can be accessed by screen readers, and by extension the potential for braille devices, that provide access for the visually impaired</li>
           <li>multiple language subtitles can be applied to instances of one source, unaltered AV file</li>      
      </ul>
      
  
    <div class="div-box1">
        <img src="images/video_CC.png" alt="image of video displaying Closed Captions" width="600px;" class="float-left"><p class="img-cap">Image left: Video displaying (Closed) Captions, the [CC] toggle switch set within the default player controls at the bottom right of the player window</p>
   </div>
   
   
  <h4>Formatting for Captions</h4>
    
    <p>Captions can simply be structured as snippets of text synchronized with the audio track for display in time-synch with video playback. They might overlay the video image within the video viewport (the portion of player window devoted to display of the video image or otherwise placed beneath. There may be no styling applied other than  display in a clear, san-serif font, at a legible font-size and in a colour that enables easy reading over the background.</p>   
    
    
    <p>Depending on the format of caption, styling can be applied to captions to distinguish by format or highlight parts of text, to clearly designate speakers and denote their speech, to position text, set background colours, colour-code speech from individual speakers and so on.</p>
    
     <div class="explain">
    <h4>Video Captioning Style Conventions</h4>
<p>There are numerous online resources describing best practice for the implementation, and styling of captions. To make the textural representation of audible and visual events consistent and easily comprehended it is best to follow 'universally' implemented best-practice recommendations. Examples of such text stylings include:</p>
       
       <ul><li>Use square brackets for non-spoken sound events e.g. <strong>[A loud crash]</strong></li>
           <li>Speakers' names written within round brackets e.g. <strong>(Tom Thumb)</strong></li></ul>
           
           <h5>Useful Online Resources</h5>
       
       <p>How to Meet WCAG (Quick Reference)<a href="https://www.w3.org/WAI/WCAG21/quickref/">https://www.w3.org/WAI/WCAG21/quickref/</a></p>
        <p>Checklist for Creating Accessible Videos<a href="https://www.boia.org/blog/checklist-for-creating-accessible-videos">https://www.boia.org/blog/checklist-for-creating-accessible-videos</a></p>
         <p><a href="https://www.unimelb.edu.au/accessibility/video-captioning/style-guide">https://www.unimelb.edu.au/accessibility/video-captioning/style-guide</a></p>
        <p><a href="https://support.automaticsync.com/hc/en-us/articles/202355665-Transcription-Guidelines-for-Captioning">https://support.automaticsync.com/hc/en-us/articles/202355665-Transcription-Guidelines-for-Captioning</a></p>
        <p><a href="https://bbc.github.io/subtitle-guidelines/">https://bbc.github.io/subtitle-guidelines/</a></p>
        
        
    </div>
           
      
      
      
      
       <!--TRANSCRIPTS-->
    
    <h4>Transcripts</h4>
      <p><strong><em>How does a transcript differ from captions?</em></strong><br>Captions contain the textural output derived from speech in the audio, generally representing the full duration of an AV file. However as captions are presented sequentially within a limited display space, they appear in a 'piece-meal' fashion as small blocks of text made visible to the user as the corresponding speech is rendered audible. The block is then removed to be replaced by a succeeding one for the next part of speech. When presented within the shell of the Media Player in this time-dependent manner they can be reviewed by playback control but are not directly available for search and generally the viewer must play back the entirety of the video in order to see the whole caption set.</p>    
      <p>In presentation  <strong><em>transcripts</em></strong> contain the full speech from a file's audio made immediately available to the user in its entirety, unbounded by time-constraint. Whereas the render of captions and subtitles is time-code dependent transcripts, in their simplest form, can be entirely time-independent (- see interactive transcripts). They are presented as separate documents generally outside of the media player's shell. </p>
      <p>Depending on format and arrangement they are generally searchable in some capacity, -either from web-browser text-search, or via a designated search-tool or script, and the cotent available to edit, so that text may be quoted from or employed elsewhere in other scenarios.</p>
      
      <p>A transcript can be presented in various formats:</p>
      <ul>
          <li>simple text (*.txt or similar)</li>
          <li>Word or other document</li>
          <li>PDF</li>
          <li>HTML</li>
          <li>XML or JSON</li>
          <li>HTML5 <span class="span-code">&lt;track&gt;</span> element targeting an external source file:  WebVTT, TTML captions (XML-based, more complex and generally within broadcast domain), or caption sources in SUB, SRT and Youtube's SBV via 'plugin' support through <a href="https://captionatorjs.com/">Captionator.js</a></li>
      </ul>
    
   
       <h3>Benefits of Transcribing AV</h3>
<p>Producing captions or subtitles, and a full transcript for presentation alongside AV content has at least these benefits:</p>

 <h4>Benefits to User </h4>
 
 <ul>
    <li>captions and transcriptions improve accessibility for the hearing or visually impaired, and are considered 'alternative content' manifestations</li>
    <li>where speech is fast, unclear or with a high level background noise captions and text transcripts can assist. Particularly for the hearing impaired, background noise can be a great problem when attempting to isolate a single voice of interest (- the relative ability of transcribing systems to extract correct information in this scenario is a major point of interest)</li>
    <li>experienced users of 'assistive-technology' such as screen readers that are capable of accessing a text transcript may be able to take content in less time in this manner than by listening to the audio</li>
    <li>subtitles improve accessibility for viewers who don’t speak the language used in the AV file, and are considered 'alternative content' manifestations. In the case of content made available on the web, this helps worldwide users in addition to domestic users</li>
    <li>navigation of video playback for all users can be enabled through the implementation of <strong><em>'interactive transcripts'</em></strong></li>
    <li>navigation for visually impaired users - as sighted users can navigate along the video time-line by time-display, preview thumbnails in the player's playback progress bar, and possibly via <strong><em>'Chapters'</em></strong> this functionality could potentially be enabled for the visually impaired by screen-reader search of transcript text, with an opportunity to then jump to that point in playback </li>
     <li>a transcript can be searched from a browser's page-content search or via a JavaScript or other search tool.</li>
    <li>when inconvenient to view video content, a text transcript can be an instant-access alternative with significantly less time or resource required for render and display. A user may scan through a text-transcript to quickly get to the desired information.</li>
     <li>the ability to read along with video playback may allow for better comprehension, and so long as the transcript is accurate, helps to ensure the correct interpretation of speech</li>
    </ul>
 
  <h4>TNA-Centric Benefits from the Production of Transcripts</h4>
  
  <ul>
  <li>production of text transcriptions negates the need to be able to play-back video where the audio source is the primary information component. Transcript text can be used wholly independently of the video source, and can be utilised or presented in different scenarios</li>
  
  <li>speech rendered to text makes the information more portable and can be rendered in just about any format and digital/ print environment. Originating from one the largest file-sized formats (i.e. High Definition + video) speech-to-text output renders to one of the most compact and universally interoperable</li>
  
   <li>the availability of accurate text transcripts could simplify the sensitivity review of associated AV materials, both for 'on-hand'/ already accessioned content, and for incoming</li>
  
  <li>there is the potential to perform searches across an entire video collection,- and in combination with document formats, when employing a search across text transcripts. 
     <ul>
     <li>Searches could be made in order to pick out references across a set of files</li> 
     <li>and to help identify any duplicate sequences</li>
     </ul> 
 </li>     
 


<li>captions and transcripts allow for the provision of flexible 'viewing' arrangements for 'sound-sensitive environments' such as Reading Rooms. AV captions, or preferably independent transcripts, negate the necessity for sound for hearing-impaired or visually-impaired users when a suitable screen reading system is made available</li>

      <li>a transcript can makes an online video more findable as text content is better indexed by search engines</li>
 
<li>for online video, subtitles (in multiple languages) 

<ul>
    <li>assist worldwide users in addition to domestic users of different first-languages</li> 
    <li>helps to enhance TNA's international influence</li>
</ul>
</ul>
   



    
   <!--TODO-->
   <h4>Steps to add interactive transcripts</h4>
   
   <ol>
   <li>Submit Your audio or video for (automated) transcription service</li>
   <li>Download Your caption file in WebVTT Format</li>
   <li>Add the caption to the &gt;track&gt; child element of the HTML5 &gt;video&gt; element</li>
   <li>Add 'vanilla' JavaScript or jQuery to enable interaction with DOM elements involved in the actioning of transcript-interaction. Alternatively use a pre-built 'JS plugin', (such as <a href="https://github.com/walsh9/videojs-transcript">https://github.com/walsh9/videojs-transcript</a>)   either downladed or externally called by <span class="span-code">&lt;script&gt;</span> reference</li>
     </ol> 
   
    

    <h3>Accessibility Opportunities for Video Presentation</h3>

    <h4>Descriptive Captions</h4>
    <p><em><strong><em>'Descriptive captions'</em></strong></em> include speech along with descriptive notes of visual events that occur within the video action, and can be employed to identify each speaker. These enhanced captions can be employed to provide the hearing-impaired user with a text-description of those events. These can also be delivered via a screen-reader to a visually-impaired user.  and interact.</p>

    <h4>Interactive Transcripts in Conjunction with Screen Readers</h4>
    <p>Interactive transcripts can also be used for navigation for vision-impaired users when in combination with a screen-reader (it would probably
      be necessary to mute the audio-track in this case so that the it doesn't compete with the speech generated by the screen reader.</p>
      
        <p>A 'Tab-Index' could be used to enable navigation control of a scrollable transcript. Each cue could be assigned a <strong><em>'@data-time'</em></strong>, which carries a start-time along with a @tabindex, allowing vision-impaired users to navigate by pressing the Tab key.</p>
        
        <h4>Aria-Live for Screen-Readers</h4> 
    <p>When a portion of a web page is updated, then that 'region' of the web-page that changes is denoted a 'live region'. An <strong><em>'aria-live'</em></strong>  attribute can be added to an active page element to relate to a screen-reader that a change has been made that should be read-out. This process can be used to related an update to a text area, so that an aria-live attribute informs the screenreader as to what text descriptions within an element have changed and so these can be related to the user.</p>
                  
    <h4>Variable Speed of Playback</h4>
    <p>Reportedly a typical Braille reading speed is around 60 words per minute (an 'average adult reading speed' for sighted readers is set at around 250 to 300
      words per minute). In regular speech it is common to deliver around 130-200 words per minute. Conversely some visually-impaired users may have an enhanced capacity to take in audio at speed, and thus comprehend the content of otherwise lengthy durations of AV at a significantly higher rate. As such it would be beneficial to provide an easily accessible control by which the individual user could set an appropriate speed to best meet their needs. The video player model: 
      <a href="Player_Models.html#int-trans-chap-player">Model AV Player with Interactive Transcript &amp; Chapters)</a> incorporates a <em>'Play Speed'</em> control, in the form of a slider.
    </p>
    

    <h4>
      Alpha Channel Video for Overlay of Sign Language</h4>
    <p>For the benefit of profoundly hearing-impaired users there is the potential to use a 'video overlay', where a video of speech being interpreted
      by a signer, is placed over the source video but where the background is rendered invisible by manipulation of the 'alpha channel' of the image.
      Alpha channels are masks through which you can display images beneath.</p>
    <p>
      The alpha channel is an 8-bit channel, which gives 256 levels of grey from 0 (black) to 255 (white). White then represents the visible area, and
      black the transparent area (you see the background behind the image when displayed). The levels of grey between white and black values determines
      the relative level of visibility opacity, e.g: 50 percent grey creates 50 percent visibility. Alpha channels are usually employed with colour RGB images
      where the resulting image is then termed 'RGBA' (= RGB +A, where 'A' refers to the Alpha channel).
      <a href="https://twitter.com/i/status/1129778677087166464">Sign language overlay example from NHS page.</a>
    </p>

    <h4>Refreshable Braille Displays</h4>
    <p>A 'refreshable braille display' or 'braille terminal' is a device for displaying braille characters, usually in the form of round-tipped pins
      which are raised through holes in a flat surface in arrangement to form braille characters. This allows profoundly visually-impaired users or those who are both
      profoundly hearing and visually impaired to read text output (as supplied by screen-reading software.</p>

    
    
              
              
    
  

    
    <br><br><br><br>

                    
              
                    
 
  </div>
  </div>
  <!--BODY-->
  
  <!--<div id="text1" style="top: 100px; width: 200px; ">
<p>
  I'm here!
</p>
        </div>-->
<!--SCRIPT-->
        <script type="text/javascript">
        function unhide(playpricing) {
        var item = document.getElementById(playpricing);
        if (item) {
        item.className=(item.className=='hidden')?'unhidden':'hidden';
        }
        }
        </script> 


</body>

</html>
